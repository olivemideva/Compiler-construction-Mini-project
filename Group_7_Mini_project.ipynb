{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olivemideva/Compiler-construction-Mini-project/blob/main/Group_7_Mini_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group 7**\n",
        "\n",
        "128741 Yuda Betty Makena\n",
        "\n",
        "135734 Chelsea Mogore\n",
        "\n",
        "135792 Muloma Olive Mideva\n",
        "\n",
        "136805 Abigail Muthuri\n",
        "\n",
        "120351 Mburu Evanson\n",
        "\n",
        "127135 Atulah Harry\n",
        "\n",
        "111218 Shyleen Mwadeghu\n",
        "\n",
        "136268 Saidi Walid Iddi\n"
      ],
      "metadata": {
        "id": "XpwdFwbsAQY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "o0FpGVWszdZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "9HMbZ2jmwKkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining token patterns using regular expressions**"
      ],
      "metadata": {
        "id": "5lykmpOrwZVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_patterns = [\n",
        "    (r'\\d+', 'NUMBER'),          # Match integers\n",
        "    (r'\\+', 'PLUS'),            # Match addition operator\n",
        "    (r'-', 'MINUS'),            # Match subtraction operator\n",
        "    (r'\\*', 'MULTIPLY'),        # Match multiplication operator\n",
        "    (r'/', 'DIVIDE'),           # Match division operator\n",
        "    (r'\\(', 'LPAREN'),          # Match left parenthesis\n",
        "    (r'\\)', 'RPAREN'),          # Match right parenthesis\n",
        "]"
      ],
      "metadata": {
        "id": "LeUiNVEewYCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Token class to represent indivual tokens with type and value**"
      ],
      "metadata": {
        "id": "DY4GmlSvwni3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Token:\n",
        "    def __init__(self, type, value):\n",
        "        self.type = type\n",
        "        self.value = value"
      ],
      "metadata": {
        "id": "F_5szX2Cwkoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lexer function that tokenizes input expression**"
      ],
      "metadata": {
        "id": "CiMIoZ_SxA4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lexer(input_str):\n",
        "  # Initialize an empty list to store tokens\n",
        "    tokens = []\n",
        "    while input_str:\n",
        "      # Initialize match as None before attempting to match any pattern\n",
        "        match = None\n",
        "        for pattern, token_type in token_patterns:\n",
        "            regex = re.compile(pattern)\n",
        "            # Attempt to match the pattern at the beginning of the input string\n",
        "            match = regex.match(input_str)\n",
        "            if match:\n",
        "                value = match.group(0)\n",
        "                # Create a Token object with the token type and value, and append it to the list of tokens\n",
        "                tokens.append(Token(token_type, value))\n",
        "                input_str = input_str[len(value):].lstrip()\n",
        "                break\n",
        "        if not match:\n",
        "          # Raise a ValueError indicating an unexpected character at the beginning of the input string\n",
        "            raise ValueError(f\"Lexer error: Unexpected character '{input_str[0]}'\")\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "uIowROCyw_TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parser used for arithmetic expressions**"
      ],
      "metadata": {
        "id": "rBy7syM8xNly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_expression(tokens):\n",
        "    return parse_addition(tokens)\n"
      ],
      "metadata": {
        "id": "lPUOHFXWxMUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper function for parsing addition and subtraction**"
      ],
      "metadata": {
        "id": "Z0_6A3mpxfeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_addition(tokens):\n",
        "   # Parse the left-hand side of the addition/subtraction expression\n",
        "    left = parse_multiplication(tokens)\n",
        "    while tokens and tokens[0].type in ('PLUS', 'MINUS'):\n",
        "\n",
        "        # Pop the operator token (PLUS or MINUS)\n",
        "        operator = tokens.pop(0)\n",
        "   # Parse the right-hand side of the addition/subtraction expression\n",
        "        right = parse_multiplication(tokens)\n",
        "        left = {'type': operator.type, 'left': left, 'right': right}\n",
        "    return left"
      ],
      "metadata": {
        "id": "Cu1dx5_mxadH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper function for parsing multiplication and division**"
      ],
      "metadata": {
        "id": "QuZTkrJVzEKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_multiplication(tokens):\n",
        "  # Parse the left-hand side of the multiplication/division expression\n",
        "    left = parse_primary(tokens)\n",
        "    # Continue parsing while there are tokens and the next token is a MULTIPLY or DIVIDE operator\n",
        "    while tokens and tokens[0].type in ('MULTIPLY', 'DIVIDE'):\n",
        "        operator = tokens.pop(0)\n",
        "        # Parse the right-hand side of the multiplication/division expression\n",
        "        right = parse_primary(tokens)\n",
        "        # Create a dictionary node representing the multiplication/division operation\n",
        "        left = {'type': operator.type, 'left': left, 'right': right}\n",
        "    return left"
      ],
      "metadata": {
        "id": "fS9zYDcCzAbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper function for parsing primary expressions (numbers and parentheses)**"
      ],
      "metadata": {
        "id": "VzPficgNxptL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_primary(tokens):\n",
        "  # Check if the first token is a NUMBER\n",
        "    if tokens[0].type == 'NUMBER':\n",
        "        return int(tokens.pop(0).value)\n",
        "\n",
        "        # If the first token is an LPAREN (left parenthesis)\n",
        "    elif tokens[0].type == 'LPAREN':\n",
        "        tokens.pop(0)\n",
        "        expression = parse_addition(tokens)\n",
        "\n",
        "        # Check if the next token is an RPAREN (right parenthesis)\n",
        "        if tokens[0].type == 'RPAREN':\n",
        "            tokens.pop(0)\n",
        "            return expression\n",
        "        else:\n",
        "            raise ValueError(\"Parser error: Unmatched parenthesis\")\n",
        "    else:\n",
        "       # If the first token is neither NUMBER nor LPAREN, raise a ValueError\n",
        "        raise ValueError(f\"Parser error: Unexpected token '{tokens[0].value}'\")"
      ],
      "metadata": {
        "id": "BJpsh04Oxldo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# Evaluate the abstract syntax tree (AST)**"
      ],
      "metadata": {
        "id": "KhPdnOuaxz2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(ast):\n",
        "  # Check if the AST node is already an integer (base case)\n",
        "    if type(ast) == int:\n",
        "        return ast\n",
        "        # If the AST node represents an addition operation\n",
        "    if ast['type'] == 'PLUS':\n",
        "        return evaluate(ast['left']) + evaluate(ast['right'])\n",
        "        # If the AST node represents a subtraction operation\n",
        "    elif ast['type'] == 'MINUS':\n",
        "        return evaluate(ast['left']) - evaluate(ast['right'])\n",
        "         # If the AST node represents a multiplication operation\n",
        "    elif ast['type'] == 'MULTIPLY':\n",
        "        return evaluate(ast['left']) * evaluate(ast['right'])\n",
        "        # If the AST node represents a division operation\n",
        "    elif ast['type'] == 'DIVIDE':\n",
        "        return evaluate(ast['left']) / evaluate(ast['right'])"
      ],
      "metadata": {
        "id": "R75L_xwmxwVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example usage**"
      ],
      "metadata": {
        "id": "DuE-mgzlx78q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_expression = \"3 + 5 * ( 10 - 2 )\"\n",
        "# Tokenize the input expression using the lexer function\n",
        "tokens = lexer(input_expression)\n",
        "\n",
        "# Parse the tokenized input into an abstract syntax tree (AST)\n",
        "parsed_ast = parse_expression(tokens)\n",
        "\n",
        "# Evaluate the AST to obtain the final result\n",
        "result = evaluate(parsed_ast)\n",
        "print(f\"Expression: {input_expression}\")\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u02EBY4_x4Ti",
        "outputId": "7165c55f-4286-4e6e-f2b6-03fad7317182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression: 3 + 5 * ( 10 - 2 )\n",
            "Result: 43\n"
          ]
        }
      ]
    }
  ]
}