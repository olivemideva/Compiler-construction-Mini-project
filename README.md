# Compiler-construction-Mini-project
GROUP 7
##
128741 Yuda Betty Makena
##
135734 Chelsea Mogore
##
135792 Muloma Olive Mideva
##
136805 Abigail Muthuri
##
120351 Mburu Evanson
##
127135 Atulah Harry
##
111218 Shyleen Mwadeghu
##
136268 Saidi Walid Iddi
##

# Lexer (Tokenization):
Logic: The lexer is responsible for breaking down the input string into a sequence of tokens, which are the basic building blocks of the language (e.g., numbers, operators, parentheses). The logic of the lexer involves pattern matching and iterative processing of the input string. It iterates through the input string and matches it against the defined token patterns, producing a list of tokens with their corresponding types and values.
Concepts:
Lexical Analysis: The lexer demonstrates the concept of lexical analysis, which is the first phase of a compiler. It recognizes and categorizes tokens in the input source code.
Tokenization: The tokens generated by the lexer correspond to the terminal symbols of the language's grammar. This aligns with the concept of tokenization, where source code is divided into meaningful units.
# Parser (Syntax Analysis):
Logic: The parser is responsible for taking the list of tokens produced by the lexer and constructing an abstract syntax tree (AST) based on the grammar of the arithmetic expression language. The logic involves defining a grammar for expressions that includes addition, subtraction, multiplication, and division. The parser uses recursive descent parsing, a top-down parsing technique, to create the AST. The AST represents the hierarchical structure of the expression.
Concepts:
Syntax Analysis (Parsing): The parser demonstrates the concept of syntax analysis, which is the second phase of a compiler. It ensures that the sequence of tokens adheres to the specified grammar rules, creating a structured representation of the code.
Abstract Syntax Tree (AST): The generated AST represents the hierarchical structure of the parsed expression. ASTs are a core concept in compilers and interpreters, as they facilitate further processing and execution of code.
# Token Patterns and Regular Expressions:
Logic: Token patterns are defined using regular expressions. These patterns describe the structure of valid tokens in the language. The regular expressions are used by the lexer to match and extract tokens from the input string.
Concepts:
Regular Expressions: Regular expressions are a key concept in lexical analysis. They define patterns for identifying tokens in the input source code.
# Error Handling:
Logic: The code includes error handling for cases such as unexpected characters during tokenization and unmatched parentheses during parsing. Error messages are raised to indicate issues in the input expression.
Concepts:
Error Handling: Error handling is an essential aspect of language processing. It ensures that the code behaves predictably in the presence of unexpected or erroneous input.
# Modularity:
Logic: The code is organized into functions for tokenization, parsing, and evaluation, making it modular and easy to understand. Each function has a specific responsibility, promoting maintainability and readability.
Concepts:
Modularity: Modularity in code design is a good practice. It allows for clear separation of concerns, making the code easier to manage and extend.
# Abstract Syntax Tree Evaluation:
Logic: After parsing, the code provides an evaluate function that recursively computes the result of the expression by traversing the AST. It performs arithmetic operations based on the node types in the AST.
Concepts:
Abstract Syntax Tree (AST): ASTs are a central concept in compiler design and interpretation. They provide a structured representation of code that can be evaluated or translated into machine code.
# Code Organization and Example Usage:
Logic: The code is organized into functions, and an example expression is tokenized, parsed, and evaluated. The result is printed to the console, demonstrating the functionality of the lexer and parser.
Concepts:
Code Organization: A well-structured code organization makes the implementation more readable and maintainable.
In summary, the mini project effectively demonstrates the concepts of lexical analysis, syntax analysis, regular expressions, abstract syntax trees, error handling, and code modularity. It provides a practical example of how lexers and parsers work in the context of a simple arithmetic expression language.
##
# Description of Details and Logic Used:
##
1. Tokenization (Lexer): The code defines a set of regular expressions to match different types of tokens in the input expression, including numbers, operators, and parentheses. These regular expressions are used to break down the input string into a list of `Token` objects, where each token has a type and a value. The logic used in the lexer involves pattern matching and iterative processing of the input string.

2. Parsing (Parser): The code implements a recursive descent parser to construct an abstract syntax tree (AST) based on the grammar of arithmetic expressions. It starts with the highest-precedence operators and works down to lower-precedence ones. The parser recursively parses expressions by breaking them down into smaller sub-expressions. The logic used in the parser involves handling different operators, precedence levels, and ensuring proper syntax according to the grammar.

3. Evaluation: After parsing, the code provides an `evaluate` function that recursively computes the result of the expression by traversing the AST. It performs arithmetic operations based on the node types in the AST, which corresponds to the structure of the parsed expression.

4. Error Handling: The code includes error handling for cases such as unexpected characters during tokenization and unmatched parentheses during parsing. This demonstrates robustness and proper handling of edge cases.

5. Modularity: The code is organized into functions for tokenization, parsing, and evaluation, making it modular and easy to understand. Each function has a specific responsibility, promoting maintainability and readability.
##
# A Show of Connection Between the Solution and the Concept(s) Learned in Class (4 marks):
##
1. Lexer and Tokenization: The code demonstrates the concept of lexical analysis by using regular expressions to tokenize input expressions. This is a fundamental step in the compiler design process, where source code is broken down into tokens for further processing.

2. Parser and Syntax Analysis: The code illustrates the concept of syntax analysis (parsing) by implementing a recursive descent parser. It enforces the grammar rules of arithmetic expressions, ensuring that the input adheres to the specified syntax. This aligns with the process of parsing source code to create a structured representation.

3. Abstract Syntax Tree (AST): The code constructs an abstract syntax tree (AST) to represent the hierarchical structure of the parsed expression. ASTs are widely used in compilers and interpreters to represent program structure. The code then evaluates the AST to compute the result, showcasing the practical relevance of ASTs.

4. Error Handling: Error handling is an essential aspect of compiler design and language processing. The code handles lexer and parser errors, which is a crucial concept in ensuring the correctness of source code.

In summary, the code effectively connects the implementation of a lexer and parser for arithmetic expressions with the concepts of lexical analysis, syntax analysis, abstract syntax trees, and error handling learned in class. It provides a practical example of these concepts in action.

